# -*- coding: utf-8 -*-
"""20191597_박유영_전반부 최종 프로젝트.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1st6YRSpEMeQStaa3MmyQkmHIF0A02ssX
"""

!pip install mediapipe

from google.colab import files

uploaded = files.upload()

import cv2
from google.colab.patches import cv2_imshow

# Read images with OpenCV.
images = {name: cv2.imread(name) for name in uploaded.keys()}

# Preview the images.
for name, image in images.items():
  print(name)   
  resize_img = cv2.resize(image, dsize=(480, 640), interpolation=cv2.INTER_AREA)
  cv2_imshow(resize_img)

import mediapipe as mp
mp_hands = mp.solutions.hands

mp_drawing = mp.solutions.drawing_utils 

with mp_hands.Hands(
    static_image_mode=True,
    max_num_hands=2,
    min_detection_confidence=0.7) as hands:
  for name, image in images.items():
    # Convert the BGR image to RGB, flip the image around y-axis for correct 
    # handedness output and process it with MediaPipe Hands.
    resize_image = cv2.resize(image, dsize=(480, 640), interpolation=cv2.INTER_AREA)
    results = hands.process(cv2.flip(cv2.cvtColor(resize_image, cv2.COLOR_BGR2RGB), 1))
    image_hight, image_width, _ = resize_image.shape
    # Print handedness (left v.s. right hand).
    print(f'Handedness of {name}:')
    print(results.multi_handedness)

    # Draw hand landmarks of each hand.
    print(f'Hand landmarks of {name}:')
    if not results.multi_hand_landmarks:
      continue
    annotated_image = cv2.flip(resize_image.copy(), 1)
    for hand_landmarks in results.multi_hand_landmarks:
      # Print index finger tip coordinates.
      print(
          f'Index finger tip coordinate: (',
          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '
          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_hight})'
      )
      mp_drawing.draw_landmarks(
          annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
    cv2_imshow(cv2.flip(annotated_image, 1))

from google.colab import files

uploaded2 = files.upload()

import os
import matplotlib.pylab as plt
import mediapipe as mp
cap = cv2.VideoCapture("/content/stop_go_stop.mp4") #자신의 웹캠 장치 번호를 적어야 한다. 0은 첫번째 장치를 사용하겠다는 것

mpHands = mp.solutions.hands
hands = mpHands.Hands()
mpDraw = mp.solutions.drawing_utils

while (cap.isOpened()):
    success, img = cap.read()
    status = "" # 현재 자동차의 주행 상태

    frame_width = int(cap.get(3))
    frame_height = int(cap.get(4))

    out = cv2.VideoWriter("/content/result/result.avi", cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (frame_width, frame_height))

    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

    #핸드를 인식하면 처리 되는 코드
    if success == True:
      if results.multi_hand_landmarks:
        for handLandmarks in results.multi_hand_landmarks:
          #핸드의 각 관절 포인트의 ID와 좌표를 알아 내서 원하는 그림을 그려 넣을 수 있다. 
          for id, lm in enumerate(handLandmarks.landmark):
            h, w, c = img.shape
            cx, cy = int(lm.x * w), int(lm.y * h)
            #손목관절
            if id == 0:
              cv2.circle(img, (cx, cy), 10, (180, 255, 255), cv2.FILLED)
            #중지 끝
            if id == 8:
              cv2.circle(img, (cx, cy), 15, (90, 180, 0), cv2.FILLED)
                      
              if lm.x < 0.42 and lm.z < 0:
                status = "stop"
              elif lm.x >= 0.42 and lm.z >= 0:
                status = "go"
              else:
                status = "?"

          #인식된 핸드에 점과 선을 그려 넣는다.
          mpDraw.draw_landmarks(img, handLandmarks, mpHands.HAND_CONNECTIONS)

      cv2.putText(img, status,(10, 70), cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 255), 3)

      out.write(img)

      cv2_imshow(img)
      cv2.waitKey(1)
    
    else:
      break

cap.release()
out.release()

